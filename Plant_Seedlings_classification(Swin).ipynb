{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp3pAJQhgI3m4jULZQu9Fx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siting1206/Plant_seedlings_classification/blob/main/Plant_Seedlings_classification(Swin).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLZa7Uq29-sp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "from timm.utils import accuracy, AverageMeter\n",
        "from sklearn.metrics import classification_report\n",
        "from timm.data.mixup import Mixup\n",
        "from timm.loss import SoftTargetCrossEntropy\n",
        "from torchvision import datasets\n",
        "from timm.models.swin_transformer_v2 import swinv2_tiny_window8_256\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#Define EMA\n",
        "class EMA():\n",
        "    def __init__(self, model, decay):\n",
        "        self.model = model\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "\n",
        "    def register(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.shadow\n",
        "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "                self.shadow[name] = new_average.clone()\n",
        "\n",
        "    def apply_shadow(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.shadow\n",
        "                self.backup[name] = param.data\n",
        "                param.data = self.shadow[name]\n",
        "\n",
        "    def restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.backup\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # create the file to save the model\n",
        "    file_dir = 'checkpoints/SwinV2'\n",
        "    if os.path.exists(file_dir):\n",
        "        print('true')\n",
        "        # os.rmdir(file_dir)\n",
        "        shutil.rmtree(file_dir)  # remove the old one, then create a new one\n",
        "        os.makedirs(file_dir)\n",
        "    else:\n",
        "        os.makedirs(file_dir)\n",
        "    # set global parameters\n",
        "    model_lr = 1e-4\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 300\n",
        "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    use_amp = False  # automatic mixed precision\n",
        "    use_dp=False \n",
        "    classes = 12\n",
        "    resume = False # Whether to continue training with the last model\n",
        "    CLIP_GRAD = 5.0\n",
        "    model_path = 'best.pth'\n",
        "    Best_ACC = 0 # record the best acc\n",
        "    use_ema=True\n",
        "    ema_epoch=32\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.GaussianBlur(kernel_size=(5,5),sigma=(0.1, 3.0)),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.32881442, 0.2893456, 0.20730409], std= [0.09346988, 0.097306244, 0.1065664])\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.32881442, 0.2893456, 0.20730409], std= [0.09346988, 0.097306244, 0.1065664])\n",
        "])\n",
        "# Define data augmentation mixup\n",
        "mixup_fn = Mixup(\n",
        "    mixup_alpha=0.8, cutmix_alpha=1.0, cutmix_minmax=None,\n",
        "    prob=0.1, switch_prob=0.5, mode='batch',\n",
        "    label_smoothing=0.1, num_classes=classes)\n",
        "\n",
        "dataset_train = datasets.ImageFolder('data/train', transform=transform)\n",
        "dataset_test = datasets.ImageFolder(\"data/val\", transform=transform_test)\n",
        "with open('class.txt', 'w') as file:\n",
        "    file.write(str(dataset_train.class_to_idx))\n",
        "with open('class.json', 'w', encoding='utf-8') as file:\n",
        "    file.write(json.dumps(dataset_train.class_to_idx))\n",
        "# load train/test data\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# set the loss function\n",
        "criterion_train = SoftTargetCrossEntropy()\n",
        "criterion_val = torch.nn.CrossEntropyLoss()\n",
        "# set the model-swinv2\n",
        "model_ft = swinv2_tiny_window8_256(pretrained=True)\n",
        "print(model_ft)\n",
        "num_ftrs = model_ft.head.in_features\n",
        "model_ft.head = nn.Linear(num_ftrs, classes)\n",
        "print(model_ft)\n",
        "if resume:  # If resume is True, the model in model_path  continues to train\n",
        "    model_ft = torch.load(model_path)\n",
        "# If cuda is available, move to gpu\n",
        "model_ft.to(DEVICE)\n",
        "\n",
        "# choose the optimizer AdamW(Adam + L2)\n",
        "optimizer = optim.AdamW(model_ft.parameters(), lr=model_lr)\n",
        "# the learning rate adjustment strategy is chosen as cosine\n",
        "cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-6)\n",
        "if use_amp:\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "if torch.cuda.device_count() > 1 and use_dp:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model_ft = torch.nn.DataParallel(model_ft)\n",
        "if use_ema:\n",
        "    ema = EMA(model_ft, 0.999)\n",
        "    ema.register()\n",
        "    \n",
        "    \n",
        "# Define train\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    # AverageMeter(): Computes and stores the average and current value\n",
        "    loss_meter = AverageMeter()\n",
        "    acc1_meter = AverageMeter()\n",
        "    acc5_meter = AverageMeter()\n",
        "    total_num = len(train_loader.dataset)\n",
        "    print(total_num, len(train_loader))\n",
        "    # mixup_fn only accepts even\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if len(data) % 2 != 0:\n",
        "            if len(data) < 2:\n",
        "                continue\n",
        "            data = data[0:len(data) - 1]\n",
        "            target = target[0:len(target) - 1]\n",
        "            print(len(data))\n",
        "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "        samples, targets = mixup_fn(data, target)\n",
        "        output = model(samples)\n",
        "        optimizer.zero_grad()\n",
        "        if use_amp:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                loss = criterion_train(output, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "            # Unscales gradients and calls\n",
        "            # or skips optimizer.step()\n",
        "            scaler.step(optimizer)\n",
        "            # Updates the scale for next iteration\n",
        "            scaler.update()\n",
        "            if use_ema and epoch%ema_epoch==0:\n",
        "                ema.update()\n",
        "        else:\n",
        "            loss = criterion_train(output, targets)\n",
        "            loss.backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "            optimizer.step()\n",
        "            if use_ema and epoch%ema_epoch==0:\n",
        "                ema.update()\n",
        "        torch.cuda.synchronize() # Wait for all the above operations to complete\n",
        "        lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "        loss_meter.update(loss.item(), target.size(0))\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        loss_meter.update(loss.item(), target.size(0))\n",
        "        acc1_meter.update(acc1.item(), target.size(0))\n",
        "        acc5_meter.update(acc5.item(), target.size(0))\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR:{:.9f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item(), lr))\n",
        "    ave_loss =loss_meter.avg\n",
        "    acc = acc1_meter.avg\n",
        "    print('epoch:{}\\tloss:{:.2f}\\tacc:{:.2f}'.format(epoch, ave_loss, acc))\n",
        "    return ave_loss, acc\n",
        "\n",
        "\n",
        "# Define validation\n",
        "@torch.no_grad() # The data does not need to compute gradients, nor does backpropagation\n",
        "def val(model, device, test_loader):\n",
        "    global Best_ACC\n",
        "    model.eval()\n",
        "    loss_meter = AverageMeter()\n",
        "    acc1_meter = AverageMeter()\n",
        "    acc5_meter = AverageMeter()\n",
        "    total_num = len(test_loader.dataset)\n",
        "    print(total_num, len(test_loader))\n",
        "    val_list = []\n",
        "    pred_list = []\n",
        "    if use_ema and epoch%ema_epoch==0:\n",
        "        ema.apply_shadow()\n",
        "    for data, target in test_loader:\n",
        "        for t in target:\n",
        "            val_list.append(t.data.item())\n",
        "        data, target = data.to(device,non_blocking=True), target.to(device,non_blocking=True)\n",
        "        output = model(data)\n",
        "        loss = criterion_val(output, target)\n",
        "        _, pred = torch.max(output.data, 1)\n",
        "        for p in pred:\n",
        "            pred_list.append(p.data.item())\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        loss_meter.update(loss.item(), target.size(0))\n",
        "        acc1_meter.update(acc1.item(), target.size(0))\n",
        "        acc5_meter.update(acc5.item(), target.size(0))\n",
        "    if use_ema and epoch%ema_epoch==0:\n",
        "        ema.restore()\n",
        "    acc = acc1_meter.avg\n",
        "    print('\\nVal set: Average loss: {:.4f}\\tAcc1:{:.3f}%\\tAcc5:{:.3f}%\\n'.format(\n",
        "        loss_meter.avg,  acc,  acc5_meter.avg))\n",
        "    if acc > Best_ACC: \n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            torch.save(model.module, file_dir + \"/\" + 'model_' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            torch.save(model.module, file_dir + '/' + 'best.pth')\n",
        "        else:\n",
        "            torch.save(model, file_dir + \"/\" + 'model_' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            torch.save(model, file_dir + '/' + 'best.pth')\n",
        "        Best_ACC = acc\n",
        "    return val_list, pred_list, loss_meter.avg, acc\n",
        "\n",
        "\n",
        "# train/validation\n",
        "is_set_lr = False # when the epoch > a certain number, set the param to True\n",
        "log_dir = {} # record log\n",
        "train_loss_list, val_loss_list, train_acc_list, val_acc_list, epoch_list = [], [], [], [], []\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_list.append(epoch)\n",
        "    train_loss, train_acc = train(model_ft, DEVICE, train_loader, optimizer, epoch)\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "    log_dir['train_acc'] = train_acc_list\n",
        "    log_dir['train_loss'] = train_loss_list\n",
        "    val_list, pred_list, val_loss, val_acc = val(model_ft, DEVICE, test_loader)\n",
        "    val_loss_list.append(val_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "    log_dir['val_acc'] = val_acc_list\n",
        "    log_dir['val_loss'] = val_loss_list\n",
        "    log_dir['best_acc'] = Best_ACC\n",
        "    with open(file_dir + '/result.json', 'w', encoding='utf-8') as file:\n",
        "        file.write(json.dumps(log_dir))\n",
        "    print(classification_report(val_list, pred_list, target_names=dataset_train.class_to_idx))\n",
        "    if epoch < 600:\n",
        "        cosine_schedule.step()\n",
        "    else: # if epoch > 600, set the learning rate to 1e-6\n",
        "        if not is_set_lr:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group[\"lr\"] = 1e-6\n",
        "                is_set_lr = True\n",
        "    fig = plt.figure(1)\n",
        "    plt.plot(epoch_list, train_loss_list, 'r-', label=u'Train Loss')\n",
        "    # picture\n",
        "    plt.plot(epoch_list, val_loss_list, 'b-', label=u'Val Loss')\n",
        "    plt.legend([\"Train Loss\", \"Val Loss\"], loc=\"upper right\")\n",
        "    plt.xlabel(u'epoch')\n",
        "    plt.ylabel(u'loss')\n",
        "    plt.title('Model Loss ')\n",
        "    plt.savefig(file_dir + \"/loss.png\")\n",
        "    plt.close(1)\n",
        "    fig2 = plt.figure(2)\n",
        "    plt.plot(epoch_list, train_acc_list, 'r-', label=u'Train Acc')\n",
        "    plt.plot(epoch_list, val_acc_list, 'b-', label=u'Val Acc')\n",
        "    plt.legend([\"Train Acc\", \"Val Acc\"], loc=\"lower right\")\n",
        "    plt.title(\"Model Acc\")\n",
        "    plt.ylabel(\"acc\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.savefig(file_dir + \"/acc.png\")\n",
        "    plt.close(2)\n"
      ]
    }
  ]
}